<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark on</title><link>https://nickyadance.github.io/docs/spark/</link><description>Recent content in Spark on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 15 Sep 2022 22:08:09 +0800</lastBuildDate><atom:link href="https://nickyadance.github.io/docs/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Dag</title><link>https://nickyadance.github.io/docs/spark/dag/</link><pubDate>Thu, 15 Sep 2022 22:09:07 +0800</pubDate><guid>https://nickyadance.github.io/docs/spark/dag/</guid><description>Shuffle&amp;amp;Exchange
scala&amp;gt; val n2 = spark.range(1, 1000000) scala&amp;gt; val n2split = n2.repartition(7); scala&amp;gt; n2split.take(2).foreach(println) stage2 run take in one task, or in one concurrency Partitions
scala&amp;gt; val ds1 = spark.range(1, 1000000) scala&amp;gt; val ds2 = spark.range(1, 1000000, 2) scala&amp;gt; val ds3 = ds1.repartition(7) scala&amp;gt; val ds4 = ds2.repartition(9) scala&amp;gt; val ds5 = ds3.selectExpr(&amp;quot;id * 5 as id&amp;quot;) scala&amp;gt; val joined = ds5.join(ds4, &amp;quot;id&amp;quot;) scala&amp;gt; val sum = joined.selectExpr(&amp;quot;sum(id)&amp;quot;) Job3 &amp;amp; Job4 both run range() -&amp;gt; partition(), 10 tasks, producing partitions Job5 reads partitions with 9 tasks, broadcast result in memory cuz the result is small enough Job6 reads partitions with 7 tasks, run id * 5 -&amp;gt; join(in memory) Job7 run take with 1 task Reference # Nice Gitbook about Spark How To Read Spark DAGs</description></item></channel></rss>